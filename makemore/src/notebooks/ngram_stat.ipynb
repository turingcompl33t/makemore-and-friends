{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef94a5cd",
   "metadata": {},
   "source": [
    "**Loading the Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359434e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the names dataset from file\n",
    "from pathlib import Path\n",
    "\n",
    "data_dir = Path.cwd() / \"..\" / \"..\" / \"data\"\n",
    "\n",
    "\n",
    "def load_names(path: Path) -> list[str]:\n",
    "    with path.open(\"r\") as f:\n",
    "        return f.read().splitlines()\n",
    "\n",
    "\n",
    "words = load_names(data_dir / \"names.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694aa9f5",
   "metadata": {},
   "source": [
    "**A Bigram Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e1b966",
   "metadata": {},
   "outputs": [],
   "source": [
    "from makemore.ngram_stat import StatisticalNGram\n",
    "\n",
    "model = StatisticalNGram(n=2)\n",
    "model.train(words)\n",
    "\n",
    "loss = model.loss(words)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9215bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.sample(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65510404",
   "metadata": {},
   "source": [
    "**A Trigram Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23489373",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = StatisticalNGram(n=3)\n",
    "model.train(words)\n",
    "\n",
    "loss = model.loss(words)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd30485",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.sample(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00a96a0",
   "metadata": {},
   "source": [
    "**Hyperparameter Tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9e9774",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def train_test_split(\n",
    "    data: list[str], test_size: float = 0.2, random_state: int = 0\n",
    ") -> tuple[list[str], list[str]]:\n",
    "    \"\"\"Perform a train / test split of the input data.\"\"\"\n",
    "    if not 0.0 < test_size < 1.0:\n",
    "        raise ValueError(\"test size must be on (0.0, 1.0)\")\n",
    "\n",
    "    random.seed(random_state)\n",
    "\n",
    "    train, test = [], []\n",
    "    for element in data:\n",
    "        if random.random() < test_size:\n",
    "            test.append(element)\n",
    "        else:\n",
    "            train.append(element)\n",
    "\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e81d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "import itertools\n",
    "\n",
    "\n",
    "def search_grid(hp: dict[str, list[Any]]) -> list[dict[str, Any]]:\n",
    "    \"\"\"Generate the search grid for a given set of hyperparameters.\"\"\"\n",
    "    product = itertools.product(*([(k, v) for v in hp[k]] for k in hp.keys()))\n",
    "    return [{k: v for k, v in candidate} for candidate in product]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9aabe37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_split(\n",
    "    data: list[str], k: int = 3, shuffle: bool = False, random_state: int = 0\n",
    ") -> list[list[str]]:\n",
    "    \"\"\"Generate a random k-fold split.\"\"\"\n",
    "    if k < 2:\n",
    "        raise ValueError(\"k must be at least 2\")\n",
    "    random.seed(random_state)\n",
    "\n",
    "    # shuffle if desired\n",
    "    input = random.sample(data, len(data)) if shuffle else data\n",
    "\n",
    "    # split and return; the remainder is always allocated to final split\n",
    "    div, mod = divmod(len(data), k)\n",
    "    return [input[i * div : (i + 1) * div] for i in range(k - 1)] + [\n",
    "        input[div * (k - 1) : div * k + mod]\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246f31ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Generator\n",
    "\n",
    "\n",
    "def kfold_split_cv(\n",
    "    data: list[str], k: int = 3, shuffle: bool = False, random_state: int = 0\n",
    ") -> Generator[tuple[list[str], list[str]], None, None]:\n",
    "    \"\"\"Perform k-fold split of input data and return groupings for cross-validation.\"\"\"\n",
    "    splits = kfold_split(data, k, shuffle, random_state)\n",
    "    for i in range(k):\n",
    "        yield [\n",
    "            element\n",
    "            for j, split in enumerate(splits)\n",
    "            for element in split\n",
    "            if j != i\n",
    "        ], splits[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea2d64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from makemore.ngram_stat import StatisticalNGram\n",
    "\n",
    "\n",
    "def tune_hyperparameters(\n",
    "    ngram_size: int,\n",
    "    data: list[str],\n",
    "    hyperparameters: dict[str, list[Any]],\n",
    "    cv_nfolds: int = 5,\n",
    "    random_state: int = 0,\n",
    ") -> tuple[dict[str, Any], float]:\n",
    "    \"\"\"Tune hyperparameters using the provided data and report the best combination.\"\"\"\n",
    "\n",
    "    # the best loss we've encountered\n",
    "    best_loss = float(\"inf\")\n",
    "    # the best hyperparameter combination we have seen so far\n",
    "    best_hp = {}\n",
    "\n",
    "    for hp in search_grid(hyperparameters):\n",
    "        # create a model instance with the current hyperparameter set\n",
    "        model = StatisticalNGram(ngram_size, **hp)\n",
    "\n",
    "        aggregate_loss = 0.0\n",
    "        for train, test in kfold_split_cv(\n",
    "            data, k=cv_nfolds, shuffle=True, random_state=random_state\n",
    "        ):\n",
    "            # train the model on k - 1 folds\n",
    "            model.train(train)\n",
    "            # compute loss on remaining fold; accumulate loss\n",
    "            aggregate_loss += model.loss(test)\n",
    "\n",
    "        # compute the mean loss across all cv iterations\n",
    "        mean_loss = aggregate_loss / cv_nfolds\n",
    "        if mean_loss < best_loss:\n",
    "            best_loss = mean_loss\n",
    "            best_hp = hp\n",
    "\n",
    "    return best_hp, best_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26824a4f",
   "metadata": {},
   "source": [
    "**Putting it all Together**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fe0336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create our global train / test split\n",
    "train, test = train_test_split(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c266653",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\"smoothing\": [0, 1, 3, 5, 10, 20]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ba31ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_size = 2\n",
    "\n",
    "# compute best hyperparameters via search\n",
    "best_hp, best_cv_loss = tune_hyperparameters(ngram_size, train, hyperparameters)\n",
    "\n",
    "# train and evaluate with full sets\n",
    "model = StatisticalNGram(ngram_size, **best_hp)\n",
    "model.train(train)\n",
    "\n",
    "test_loss = model.loss(test)\n",
    "\n",
    "print(f\"{best_hp=}\")\n",
    "print(f\"{best_cv_loss=}\")\n",
    "print(f\"{test_loss=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f7f86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_size = 3\n",
    "\n",
    "# compute best hyperparameters via search\n",
    "best_hp, best_cv_loss = tune_hyperparameters(ngram_size, train, hyperparameters)\n",
    "\n",
    "# train and evaluate with full sets\n",
    "model = StatisticalNGram(ngram_size, **best_hp)\n",
    "model.train(train)\n",
    "\n",
    "test_loss = model.loss(test)\n",
    "\n",
    "print(f\"{best_hp=}\")\n",
    "print(f\"{best_cv_loss=}\")\n",
    "print(f\"{test_loss=}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
